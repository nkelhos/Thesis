\cleartooddpage[\thispagestyle{empty}]
\newcommand{\Like}{L}
\newcommand{\LogLike}{\mathcal{L}}
\newcommand{\LogLikeMax}{\LogLike_{\textrm{max}}}
\newcommand{\xtrue}{\mathbf{x }}
\newcommand{\xdet }{\mathbf{x'}}
\newcommand{\ltrue}{\mathbf{l }}
\newcommand{\ldet }{\mathbf{l'}}
\newcommand{\btrue}{\mathbf{b }}
\newcommand{\bdet }{\mathbf{b'}}
\newcommand{\Etrue}{\mathbf{E }}
\newcommand{\Edet }{\mathbf{E'}}
\newcommand{\ttrue}{\mathbf{t }}
\newcommand{\tdet }{\mathbf{t'}}
\newcommand{\Aeff }{A_\textrm{eff }}
\newcommand{\Edisp}{E_\textrm{disp}}
\newcommand{\Mnull}{M_\textrm{null}}
\newcommand{\Malt }{M_\textrm{alt }}
\chapter{Analysis}\label{chapter:analysis}


\section{Veritas Data}\label{veritasdata}
  The analysis in this thesis relies on three sets of VERITAS data.
  One set are observations of the Crab Nebula, another of the Galactic Center, and a third set of a dark region \nicetilde\ang{5} away from the Galactic Center.
  This dark region is referred to as  Sgr A* Off, and is located at (l,b)=(\ang{357.3396}, \ang{3.9984}).
  Sgr A* Off is located a few degrees away to avoid the bright diffuse gamma-ray emission caused by the galactic plane.
  The Galactic Center and Sgr A* Off observation regions are shown in Figure~\ref{fig:gcfieldsofview}.
  To quantify the detector efficiency, observations are taken at \ang{0.5} or \ang{0.7} offsets from each observing target, in four different directions (wobbles) along Right Ascension/Declination axes.

  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\textwidth]{images/skypointings/plot.pdf}
    \caption[VERITAS Galactic Center Pointings]{
      Fields of view for Galactic Center observations.
      Each circle marks the detection area of one telescope pointing.
      Green circles are Galactic Center observations, while dark blue circles are the Sgr A* Off observations used to construct the camera-background templates.
      The light blue region is the galactic plane.
    }
    \label{fig:gcfieldsofview}
  \end{figure}

  All three sets of data include observations from both the V5 and V6 epochs (see Section~\ref{sec:epochs}).
  All used data was taken from April 2010 to June 2016.
  The specific VERITAS data run numbers are listed in Appendix~\ref{app:runlists}.

  \begin{table}[]
    \centering
    \caption{Hours of observations taken at each source/epoch combination.}
    \label{tab:observation_times}
    \begin{tabular}{|l|l|l|l|}
      \hline
      \textbf{Epoch} & \textbf{Crab} & \textbf{Sgr A*} & \textbf{Sgr A* Off} \\ \hline
      V5             & 3.3           & 46.3            & 13.0                \\ \hline
      V6             & 5.5           & 62.7            & 4.7                 \\ \hline
      % times calculated with $VERIPY/thesis/plots/obs_times.py
    \end{tabular}
  \end{table}


  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\textwidth]{images/data_elevation_plots/plot.pdf}
    \caption[VERITAS Data Elevation Exposure]{
      Camera center elevation for the three sets of data.
      The three peaks in the Sgr A* data are from the 4 wobble positions being at different elevations.
      The North wobble observations peak at elevation \nicetilde\ang{29.75}, East and West wobbles observations at \nicetilde\ang{29.25}, and South wobble observations at \nicetilde\ang{28.75}.
    }
    \label{fig:datapointingelevations}
  \end{figure}

  There are comparativly fewer Sgr A* Off observations because this source is only used for background estimation, and telescope time is in high demand.
  There is also even fewer Crab observations, as the majority of its data is taken at higher elevations, where the telescope is has increased sensitivity to lower energies.
  
  For all of these observations, quality cuts were applied.
  This includes monitoring the telescope hardware and weather by a team of VERITAS collaboration members who take the observations.
  After the data is taken, a separate collaboration member will go through each observation to check it again, and mark any bad observations or apply time cuts, which are then removed.

  Because VERITAS epoch has a different hardware configuration, each epoch has their own separate set of effective areas, point spread functions, energy migration matricies, and camera background models.
  In addition, specific IRFs were calculated for additional data dimensions, including the frequency of night sky background photons in the camera, the telescope elevation, the event energy, and each event's distance from the camera center.
  
  After this data is collected, it is combined with a likelihood analysis, detailed in the next section.

\section{Likelihood Ratio Test}
  The likelihood ratio test determines which of two models is a statistically better fit to some data.
  It is performed by calculating the likelihood of detecting a group of events, given two potential models.
  Each model must first be folded with the PSF and Energy Dispersion, then the likelhood can be calculated.
  Once the likelhood is calculated, then the parameters of the model are varied until the maximum likelihood is found for eah model.
  These two maximum likelihoods can then be used to calculate the Test Statistic, which determines which model is statistically favored.
  
  \subsection{Likelihood Calculation}
  At its heart, a likelihood is a product of probabilities.
  For a simple scenario where the number of events observed in some detector bin are determined by poisson statistics, the likelihood $\Like$ is calculated by Equation~\ref{eqn:simple_like}.
  In it, $n$ is the number of observed events in the bin and $m$ is the average number of events predicted to be in the bin by the model begin tested.
  
  \begin{equation}\label{eqn:simple_like}
    \Like = \frac{e^{-m} m^n}{n!}
  \end{equation}
  
  The average number of events $m$ is determined by integrating over all astrophysical models in energy, sky position, and time, after folding in the PSF and the energy dispersion.
  This integration and folding is discussed later in the section.
  When combining multiple bins ordered by index $j$, these probabilities are multiplied as in Equation~\ref{eqn:simple_like_2}.
  These could be bins in sky position, energy, or time, or any combination of the three.
  Future likelihood calculations may also include shower core position on the ground, or distance to the shower, or other observables.
  
  \begin{equation}\label{eqn:simple_like_2}
    \Like = \prod_j \frac{ e^{-m_j} \; m_{j}^{n_j}}{n_{j}!}
  \end{equation}

  Equation~\ref{eqn:simple_like_2} is a general formula for calculating a binned likelihood.
  As events are grouped by bins, some information is lost, which results in a less-powerful ratio test {\color{red}(??)}.
  To preserve the test's power, the above result can be expanded into an unbinned likelihood can be derived.
  First, Equation~\ref{eqn:simple_like_2} can be rearrainged:
  
  \begin{equation}\label{eqn:simple_like_3}
    \Like = \prod_j e^{-m_j} \prod_j \frac{m_j^{n_j}}{n_{j}!}
  \end{equation}
    
  \begin{equation}\label{eqn:simple_like_4}
    \Like = e^{- \sum_j m_j} \prod_j \frac{m_j^{n_j}}{n_{j}!}
  \end{equation}
  
  Then, the size of each bin can be shrunk until there are only 1 or 0 events in each bin.
  For empty bins, the product in Equation~\ref{eqn:simple_like_4} becomes
  
  \begin{equation}\label{eqn:simple_like_4a}
    n=0 \rightarrow \frac{m_j^{n_j}}{n_j!} = \frac{m_j^{0}}{0!} = \frac{1}{1} = 1 .
  \end{equation}

  For bins with 1 event, the product in Equation~\ref{eqn:simple_like_4} becomes

  \begin{equation}\label{eqn:simple_like_4b}
    n=1 \rightarrow \frac{m_j^{n_j}}{n_j!} = \frac{m_j^1}{1!} = \frac{m_j}{1} = m_j = m_i ,
  \end{equation}

  where $i$ is the $i^{\textrm{th}}$ event, and $m_i$ is the number of predicted events at event $i$'s sky position, energy, and time.
  {\color{red}(What area does $m_i$ integrate over??)}
  Then Equation~\ref{eqn:simple_like_4} becomes Equation~\ref{eqn:simple_like_5}.
  
  \begin{equation}\label{eqn:simple_like_5}
    \Like = e^{- \sum_j m_j} \prod_i m_i
  \end{equation}
  
  Because $\Like$ is expensive to compute and differentiate (needed later for maximization), the log-likelihood $\LogLike$ is instead calculated.
  
  \begin{equation}\label{eqn:simple_like_6}
    \LogLike = \textrm{log} \left ( \Like \right ) 
  \end{equation}
  
  This is possible because both $\Like$ and $\LogLike$ are both {\color{red}increasing functions (is this why??)}, so the maximum of both will be at the same position in the parameter space.
  The log likelihood for a group of bins is then:
  
  \begin{equation}\label{eqn:simple_like_7}
    \LogLike = \textrm{log} \left ( e^{- \sum_j m_j} \prod_i m_i \right ) = - \sum_j m_j + \sum_i \textrm{log} \left ( m_i \right )
  \end{equation}
  
  When the bin size is infinitly small, $m_i$ becomes the value of the model's pdf at the position of the event, $P(i,m)$.
  
  \begin{equation}\label{eqn:simple_like_8}
    \LogLike = \textrm{log} \left ( e^{- \sum_j m_j} \prod_i m_i \right ) = - \sum_j m_j + \sum_i \textrm{log} \left ( P(i,m) \right )
  \end{equation}
    
  
  {\color{red}(how does log($m_i$) become $P_i$ in arxiv ctools equation (1)?? infinitly small bin size?? )}
  
  \subsection{Instrument Response Function Folding}\label{subsec:folding}
  The average number of counts $m$ predicted by a model is calculated by integrating over the energy, sky region, and time region being tested.
  This must be done, because the reconstruction method is not perfect, all events from the astrophysical models are diffused according to the PSF and Energy Dispersion.
  Another way of understanding this is that if a source is located in some sky bin $j$, events from that source can be reconstructed in neighboring bins $j+1$ and $j-1$, increasing the predicted number of events in those bins and decreasing the number of events in bin $j$, as determined by the PSF.
  Similarly, energy dispersion will diffuse events out among neighboring energy bins as well.
  {\color{red}(a plot or diagram might be handy here)}

  This leads to the need to define two separate coordinate systems, $\xtrue$ and $\xdet$.
  $\xtrue$ is a coordinate in true space (galactic $\ltrue$ and $\btrue$, energy $\Etrue$, and time $\ttrue$) space, \textit{before} folding is applied.
  On the other hand, $\xdet$ is the coordinate in reconstructed detector space ($\ldet$ and $\bdet$, $\Edet$, and $\tdet$), \textit{after} folding is applied.
  Equation~\ref{eqn:folding} shows this integration.
  
  \begin{equation}\label{eqn:folding}
    P_i \left( \xdet \right ) = \int_\xtrue R \left ( \xdet, \xtrue \right ) * M \left ( \xtrue \right ) d\xtrue
  \end{equation}
  
  Here, $P_i \left( \xdet \right )$ is the probability of detecting an event at detector coordinates $\xdet = \left ( \ldet, \bdet, \Edet, \tdet \right )$.
  The function $M\left ( \xtrue \right )$ is the number of counts predicted by the astrophysical models at coordinate $\xtrue$, and $R \left ( \xdet, \xtrue \right )$ is the dispersion at $\xdet$ due to $\xtrue$.
  The function $R$ (the instrument Response function) incorporates the Effective Area, PSF, and Energy Dispersion information as shown in Equation~\ref{eqn:foldingR}, and is discussed further in Sections \ref{subsec:effarea}, \ref{subsec:psf}, and \ref{subsec:edisp}.
  
  \begin{equation}\label{eqn:foldingR}
    R(\xdet,\xtrue) = \Aeff(\xtrue) * PSF(\xdet,\xtrue) * \Edisp(\Edet,\xtrue)
  \end{equation}
  
  In Equation~\ref{eqn:foldingR}, functions $\Aeff$, $PSF$, and $\Edisp$ are all calculated from tables of stored values, and are derived from simulations.
  {\color{red}(probably have to talk about how they're calculated!??)}

  The Crab Nebula point source in Section~\ref{sec:crab_analysis}, the Galactic Center point source in Section~\ref{subsec:gcpointsrc}, and the Dark Matter Halo model in Section~\ref{subsec:dmhalomodel} all have this folding applied to the number of events they predict.
  An important distinction is that this folding is only applied to these astrophysical models, and not to the camera background models.
  This is because the background models are created with data events from actual observations, which have the folding already applied, and thus are already in $\xdet$ space.
  
  \subsection{Combining Models into Hypothises}
  
  When calculating the likelihood of a bin in Equation~\ref{eqn:simple_like_1}, the average number of counts predicted in a bin may come from multiple sources.
  Some fraction may come from a background model, another fraction from a specific source model, etc.
  So in order to account for multiple models, their predicted counts in a bin must be summed first, as in Equation~\ref{eqn:combinemodels}.
  
  \begin{equation}\label{eqn:combinemodels}
    \Like = \frac{e^{- \left ( \sum_k m_k \right ) } \left ( \sum_k m_k \right )^n}{n!}
  \end{equation}

  In this, index $k$ loops over the various models that contribute events to a particular bin.
  
  Part of calculating the test statistic is assembling two hypotheses, or model groups.
  For the analyses in this thesis, simple hypotheses are created, where one or more point-source models are grouped with multiple camera background models.
  
  
  \subsection{Likelihood Maximization}\label{subsec:likemax}
  The calculation of one likelihood is the measure of one fit, while this analysis requires finding the best fit model, by maximizing the likelihood.
  This is done by changing the parameters of the model, but only in directions that increase the likelihood.
  The model function used in Equation~\ref{eqn:folding} is built from spatial, spectral, and temporal components that are multiplied together in Equation~\ref{eqn:modelparts}.

  \begin{equation}\label{eqn:modelparts}
    M(\xtrue) = M_s(\xtrue) * M_e(\xtrue) * M_t(\xtrue)
  \end{equation}
  
  In this, $M_s$ is the spatial component, $M_e$ is the spectral component, and $M_t$ is the temporal component.
  As the known point sources in this analysis have relatively constant flux, and the dark matter halo is assumed to be in equilibrium, $M_t(\xtrue)=1$.
  For the simple case of a point source with a power law spectrum, Equation~\ref{eqn:simple_pntsrc_Ms} describes its spatial distribution with a Dirac delta function.
  
  \begin{equation}\label{eqn:simple_pntsrc_Ms}
    M_{s,\textrm{point}}(\xtrue) = M_s(\ltrue,\btrue,\Etrue,\ttrue) = \lim_{a\to\infty} \frac{1}{ \abs{a} \sqrt{\pi} } e^{ - \left ( \sqrt{ (\ltrue-l_o)^2 + (\btrue-b_o)^2 }/a \right )^2 }
  \end{equation}
  
  In Equation~\ref{eqn:simple_pntsrc_Ms}, $l_o$ and $b_o$ are the sky coordinates of the point source.
  More complex $M_s$ functions may have the spatial structure also depend on energy $\Etrue$ or time $\ttrue$.
  The power law energy spectrum is described by Equation~\ref{eqn:simple_pntsrc_Me}.
  
  \begin{equation}\label{eqn:simple_pntsrc_Me}
    M_{e,\textrm{powerlaw}}(\xtrue) = M_e(\ltrue,\btrue,\Etrue,\ttrue) = N_o \left ( \frac{\Etrue}{E_o} \right )^{-\gamma}
  \end{equation}
  
  In Equation~\ref{eqn:simple_pntsrc_Me}, $N_o$ is the normalization of the flux, $E_o$ is the pivot energy, and $\gamma$ is the spectral index.
  
  When setting up this model, the different parameters $l_o$, $b_o$, $N_o$, $E_o$, and $\gamma$ can be fixed, or left free.
  Free parameters can then be altered to find the maximium likelihood, while fixed parameters are never changed.
  For the analyses described in this thesis, the specific models and their $M_s$ and $M_e$ functions are discussed in later subsections of this chapter.
  CTOOLS performs the likelihood maximization with the Levenberg-Marquardt method~\cite{marquardt1963algorithm}, though there is a method to add other algorithms.
  
  
  \subsection{Test Statistic Calculation}
  
  For searching for the presence of a source, the test statistic can be calculated from the maximized likelihood of two model sets.
  The first set, called the Null Hypothesis, is the base model set $\Mnull$ without the target model being searched for.
  The second set, called the Alternate Hypothesis, is the base model set along with the target model, referred to as $\Malt$.
  Once the maximum likelihood $\LogLike_{\textrm{max}}$ is found for these two hypotheses, the test statistic can be calculated with Equation~\ref{eqn:tscalc}.
  
  \begin{equation}\label{eqn:tscalc}
    \textrm{TS} = - 2 \; \textrm{log} \left (  \frac{ \LogLikeMax( \Mnull ) }{ \LogLikeMax( \Malt ) } \right )
  \end{equation}
  {\color{red}(Does this really need the second log? or is it a misprint everywhere??)}
  
  This TS then can be used to determine the statistical significance of the target model.
  With Wilk's theorem, the if events are simulated using the Null Hypothesis pdf many times, the resulting test statistics will form a $\Chi^2$ distribution with $n$ degrees of freedom, where $n$ is the difference in number of free parameters between the two model sets $\Malt$ and $\Mnull$~\cite{wilks1938}.
  From this distribution, a test statistic can be converted into a p-value.
  In the case of {\color{red}(??)}, the significance of the target model can be calculated as $\sqrt{\textrm{TS}}$.
  
  {\color{red}(how do we convert a TS into a p-value??)}
  % http://pulsar.sternwarte.uni-erlangen.de/black-hole/2ndschool/talks/likelihood_1.pdf
  

\section{Background Models}\label{sec:bkgmodels}
  The background models predict the amount of background counts produced by a sky without gamma rays.
  This is used to model the effect of the background (primarily proton) events, which are several orders of magnitude more populous than the gamma rays.
  Background models are produced by binning observation sources with weak or no gamma-ray emission.
  For this low-elevation analysis the observations of the dark region Sgr A* Off, described in Section~\ref{veritasdata}, were used to build these backgrounds.
  These dark region events were then binned into background models, using the method described in Section~\ref{background_production}.
  To account for the difference between the V5 and V6 observatory configurations, the background observations are divided up based on their epoch, producing a unique background template for each epoch.
  These background templates only depend on the radial distance from the camera center and the event energy.

\section{Crab Nebula Analysis}\label{sec:crab_analysis}
  To verify that the CTOOLs likelihood method is physically correct, the Crab Nebula was analyzed first, before any dark matter analysis was performed.
  As the Crab Nebula is the brightest gamma ray emitter in the sky, it has been observed extensively by VERITAS and other gamma ray telescopes.
  After searching for low-elevation Crab observations, a total of 17.1 hours of data were found.
  Since the Galactic Center only rises to around \ang{30} elevation, elevation effects would also need to be searched for.
  To uncover any low-elevation effects, time cuts were applied to this data to restrict the telescope pointing elevations to \SIrange{27.5}{32.5}{\degree}, similar to the Galactic Center data later on (see Figure~\ref{fig:datapointingelevations}).
  This resulted in \SI{3.3}{hours} of V5 and \SI{5.5}{hours} of V6 epoch data (see Table \ref{tab:observation_times}).
    
  \begin{figure}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{images/test_crab_analysis/plot_elev27_5_32_5deg_4_70TeV_wobbleall_Epochall_skymap.pdf}
    \caption[Crab Counts Skymap]
    {
      Skymap of event positions.
      No corrections are made for observing time or effective area.
    }
    \label{fig:crab_skymap}
  \end{figure}
  
  The Crab was modeled by a point source with the simple power law spectrum in Equation~\ref{eqn:crab_model}, both of which are discussed in Subsection~\ref{subsec:likemax}.

  \begin{equation}\label{eqn:crab_model}
    M(\xtrue) = M_{s,\textrm{powerlaw}}(\xtrue) * M_{e,\textrm{point}}(\xtrue) = N_o \left ( \frac{\Etrue}{E_o} \right )^{-\gamma} * \lim_{a\to\infty} \frac{1}{ \abs{a} \sqrt{\pi} } e^{ - \left ( \sqrt{ (\ltrue-l_c)^2 + (\btrue-b_c)^2 }/a \right )^2 }
  \end{equation}

  The sky position of the point source is fixed to the Crab Nebula, $(l_c,b_c) = (\ang{184.557600},\ang{-5.784180})$.
  The pivot energy $E_o$ of its spectrum was fixed to \SI{16.73}{TeV}, while the normalization $N_o$ and the spectral index $\gamma$ were free to vary during the likelihood optimization.
  Only events between \SIrange{4}{70}{TeV} are used in this test analysis.
  At an elevation of \ang{25}, the reconstruction software is able to reconstruct events as low as \SI{1.5}{TeV}.
  Below \SI{4}{TeV} however, the camera sensitivity starts to decrease in a poorly understood way, and IRFs in this region are not accurate.
  Part of this decrease is explored in Section~\ref{subsec:bkgstructure} (see Figures \ref{fig:bkgvsel_crab} and \ref{fig:bkgvsel_sgra}), but accounting for this requires an unfeasibly large set of simulations.
  At higher energies, simulations become too computationally expensive above \SI{200}{TeV} when attempting to calculate IRFs.
  In order to ensure there were enough downward-fluctuating simulations to properly populate the IRFs, the analysis was limited to a maximum event energy of \SI{70}{TeV}.
    
  % values from nkelhos@warp-zeuthen.desy.de:/afs/ifh.de/group/cta/scratch/nkelhos/dm_halo_testing/veripy/thesis/analysis/crab_test/logs/statistics.txt
  After fitting all model parameters to the events from \SIrange{4}{70}{TeV}, the best fit power law values were $ N_o = \left(3.90\pm0.71\right)*10^{-20} \frac{\textrm{photons}}{\textrm{cm}^{2} \; \textrm{s} \; \textrm{MeV} } $, $ \gamma = 2.31 \pm 0.17 $, with a Test Statistic of 408.8.
  As the alternate Crab Nebula hypothesis has 110 free parameters, and the no-crab (null) hypothesis has 108 free parameters, the Test Statistic has $ 110 - 108 = 2 $ degrees of freedom.
  
  In the standard VERITAS Eventdisplay analysis, the Crab Nebula is found to have a point source significance of \SI{21.3}{$\sigma$}.
  {\color{red}Check if they use same or different energy ranges??}
    
    
  \begin{figure}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{images/test_crab_analysis/plot_elev27_5_32_5deg_4_70TeV_wobbleall_Epochall.pdf}
    \caption[Crab Test Spectrum]
    {
      Crab Nebula spectra from various analyses and observatories.
      The solid red line is the best-fit spectra from the CTOOLS analysis described in this chapter, using only events from \SIrange{4}{70}{TeV}.
      The inner red envelope is the statistical fitting error on the solid red line.
      The outer red envelope is the combined statistical+systematic error.
      The dark blue line is the standard VERITAS Eventdisplay spectrum using the same set of observations.
      The dark blue datapoints are flux points for specific energy bins, from Eventdisplay.
      Light blue is a Crab Nebula spectrum from HESS~\cite{hess2006crab}.
      Purple is a previously published spectrum from VERITAS~\cite{veritas2015crab}.
      Orange is a spectrum from MAGIC~\cite{magic2015crab}.
      {\color{red}(Update hess/magic lines to have higher maximum energy points??)}
    }
    \label{fig:crab_test_spectra}
  \end{figure}
    
  In Figure~\ref{fig:crab_test_spectra}, the fitted Crab Nebula spectra is shown, along with literature results from earlier VERITAS, HESS, and MAGIC observations of the Crab.
  
  \begin{figure}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{images/test_crab_analysis/plot_elev27_5_32_5deg_4_70TeV_wobbleall_Epochall_showclean_spl.pdf}
    \llap{
      \makebox[13.75cm][l]{ % x position
        \raisebox{5cm}{     % y position
          {
            \setlength{\fboxsep}{0pt}
            \setlength{\fboxrule}{1pt}
            \fbox{
              \includegraphics[height=4.5cm]{images/test_crab_analysis/plot_elev27_5_32_5deg_4_70TeV_wobbleall_Epochall_profl_skymap.pdf}
            }
          }
        }
      }
    }
    \caption[Crab Profile along Galactic L]
    {
      The number of counts along a \ang{0.16}-wide-slice through the Crab along the Galactic L axis.
      Blue points are the number of observed counts, with poissonian error bars.
      The green histogram bars are the number of counts predicted by all models.
      Purple histogram bars are the number of counts predicted by only the camera-background models.
      The inset plot shows the counts map from Figure~\ref{fig:crab_skymap}, with blue squares showing the profile bin locations.
    }
    \label{fig:crab_profile_l}
  \end{figure}

  \begin{figure}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{images/test_crab_analysis/plot_elev27_5_32_5deg_4_70TeV_wobbleall_Epochall_showclean_spb.pdf}
    \llap{
      \makebox[13.75cm][l]{ % x position
        \raisebox{5cm}{     % y position
          {
            \setlength{\fboxsep}{0pt}
            \setlength{\fboxrule}{1pt}
            \fbox{
              \includegraphics[height=4.5cm]{images/test_crab_analysis/plot_elev27_5_32_5deg_4_70TeV_wobbleall_Epochall_profb_skymap.pdf}
            }
          }
        }
      }
    }
    \caption[Crab Profile along Galactic B]
    {
      The number of counts along a \ang{0.16}-wide-slice through the Crab along the Galactic B axis.
      Blue points are the number of observed counts, with poissonian error bars.
      The green histogram bars are the number of counts predicted by all models.
      Purple histogram bars are the number of counts predicted by only the camera-background models.
      The inset plot shows the counts map from Figure~\ref{fig:crab_skymap}, with blue squares showing the profile bin locations.
    }
    \label{fig:crab_profile_b}
  \end{figure}
    
  \begin{figure}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{images/test_crab_analysis/plot_elev27_5_32_5deg_4_70TeV_wobbleall_Epochall_showclean_splalt.pdf}
    \llap{
      \makebox[13.75cm][l]{ % x position
        \raisebox{6.5cm}{     % y position
          {
            \setlength{\fboxsep}{0pt}
            \setlength{\fboxrule}{1pt}
            \fbox{
              \includegraphics[height=3cm]{images/test_crab_analysis/plot_elev27_5_32_5deg_4_70TeV_wobbleall_Epochall_profa_skymap.pdf}
            }
          }
        }
      }
    }
    \caption[Crab Profile along Galactic L Off Source]
    {
      The number of counts along a \ang{0.16}-wide-slice along the Galactic L axis.
      This slice doesn't go through the Crab, but instead \ang{1} higher in Galactic B.
      As this doesn't include the Crab, this plot primarily demonstrates the camera background modeling.
      Blue points are the number of observed counts, with poissonian error bars.
      The green histogram bars are the number of counts predicted by all models (not visible as the astrophysical Crab Nebula model doesn't contribute any amount of counts in this offset slice).
      Purple histogram bars are the number of counts predicted by only the camera-background models.
      The inset plot shows the counts map from Figure~\ref{fig:crab_skymap}, with blue squares showing the profile bin locations.
    }
    \label{fig:crab_profile_l_off}
  \end{figure}

  \begin{figure}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{images/test_crab_analysis/plot_elev27_5_32_5deg_4_70TeV_wobbleall_Epochall_ep.pdf}
    \llap{
      \makebox[11cm][l]{  % x position
        \raisebox{7cm}{ % y position
          {
            \setlength{\fboxsep}{0pt}
            \setlength{\fboxrule}{1pt}
            \fbox{
              \includegraphics[height=3cm]{images/test_crab_analysis/plot_elev27_5_32_5deg_4_70TeV_wobbleall_Epochall_profe_skymap.pdf}
            }
          }
        }
      }
    }
    \caption[Crab Profile in Energy]
    {
      The number of counts in a \ang{0.6} x \ang{0.6} square centered on the Crab, vs energy.
      Blue points are the number of observed counts, with poissonian error bars.
      The green histogram bars are the number of counts predicted by all models.
      Purple histogram bars are the number of counts predicted by only the camera-background models.
      The inset plot shows the counts map from Figure~\ref{fig:crab_skymap}, with a blue square showing the profile bin location.
    }
    \label{fig:crab_profile_energy}
  \end{figure}
    
  The fitted models can also be viewed, as a check that the likelihood engine is fitting the models to the data.
  In Figure~\ref{fig:crab_skymap}, the position of all counts is shown in galactic l and b.
  In Figures \ref{fig:crab_profile_l} and \ref{fig:crab_profile_b}, the counts in the observations and models were integrated along a slice of galactic l and b.
  The counts from only the camera background models is shown, along with the counts from all camera backgrounds plus the point source.
  The difference between these two lines is then the counts from the Crab point source model.
  In Figure~\ref{fig:crab_profile_energy}, a similar plot is made, though integrated in a \ang{0.6}x\ang{0.6} square around the Crab Nebula at different energies.

  \FloatBarrier

\section{Dark Matter Likelihood Analysis}\label{sec:dmlike}
  
  As the test analysis on the Crab Nebula data shows results consistant with other analysis methods and observatories, the main dark matter analysis can begin in earnest.
  The 108 hours of Galactic Center data used is described in Section~\ref{veritasdata}.
  A skymap histogram of all observed events is shown in Figure~\ref{fig:gc_counts_skymap}.
  A histogram of all events' energies from \SIrange{4}{70}{TeV} is shown in Figure~\ref{fig:gc_counts_enhist}.
  Both of these plots have no corrections for observing time or effective area.
  After assembling the data, assembling the models is the next step.
  These include the camera background models, similar to the Crab Nebula analysis.
  
  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.65\textwidth]{images/likelihood_analysis/counts_skymap.pdf}
    \caption[Galactic Center Counts Skymap]{
      Skymap of all events used in this analysis.
      No adjustments are made here for effective area, observation time, or background rate.
    }
    \label{fig:gc_counts_skymap}
  \end{figure}
  
  \begin{figure}[h]
    \centering
    \includegraphics[width=0.65\textwidth]{images/likelihood_analysis/counts_enhist.pdf}
    \caption[Galactic Center Counts Energy Histogram]{
      Histogram of all event energies used in this analysis.
      No adjustments are made here for effective area, observation time, or background rate.
    }
    \label{fig:gc_counts_enhist}
  \end{figure}

  \FloatBarrier

  \subsection{Non-Dark Astrophysical Models}\label{subsec:gcpointsrc}
  For this analysis, a point source model was added at the position of Sgr A*.
  This is because other studies have indicated that the gamma-ray excess at Sgr A* is not consistant with a dark matter halo~\cite{gc_pnt_is_not_dm1, gc_pnt_is_not_dm2, gc_pnt_is_not_dm3}.
  Instead, a point source with the broken power law spectrum in Equation~\ref{eqn:brokenplaw} was added to the list of models.
  
  \begin{equation}\label{eqn:brokenplaw}
    \frac{dN}{dE} = N_{0} * { \left ( \frac{E}{E_{pivot}} \right ) }^{\gamma} {e}^{-\frac{E}{E_{cut}}}
  \end{equation}

  {\color{red}(State GC Point Source model as a function of $M(\xtrue)$ !??)}
  
  The initial values used in Equation~\ref{eqn:brokenplaw} are from \cite{VeritasGCRidge2015}, where the specific values are $E_{pivot}=\SI{1}{TeV}$, $E_{cut}=\SI{12.8}{TeV}$, and $\gamma=-2.1$.
  % theres also the 2016 paper http://iopscience.iop.org/article/10.3847/0004-637X/821/2/129/meta
  The normalization parameter $N_{0}$ was initially set to $2.8*{10}^{-12}\,\text{cm}^{-2}\,\text{s}^{-1}\,\text{TeV}^{-1}$, but was left free in the likelihood optimization, while $E_{pivot}$, $E_{cut}$, and $\gamma$ were all fixed.
  The normalization $N_{0}$ was left free to allow for the potential of some mixing between the point source events and any dark matter halo events, i.e. a stronger dark matter halo gamma-ray flux would result in a weaker point source flux.
  
  Diffuse emission, eliminated due to complexity??
  Hard cuts may have minimized its effect??
  If diffuse emission really needed to be modeled, it would show up in the profiles along galactic b??

  \subsection{Dark Matter Models}\label{subsec:dmhalomodel}
  Dark Matter halos are modeled by a spherically-symmetric mass-per-volume density profile.
  In this analysis, an Einasto density profile (discussed in Section~\ref{dm_spatial}) is used for the spatial component of the dark matter halo model.
  For the spectral component, each dark matter mass tested has its own spectrum produced with CLUMPY (see Section~\ref{dm_spectral}).
  For the spatial component, the halo profile $M_{s,\textrm{halo}}$ is calculated in Section~\ref{dm_spatial}.

  \subsection{Likelihood Maximization Results}
  
  \begin{table}
    \centering
    \begin{tabular}{|l|l|}
      \hline
      \textbf{DM Mass $m_\chi$} & \textbf{Halo TS} \\
      \[TeV\] & \\
      \hline 
      \input{images/likelihood_analysis/tsvals.tex}
      \hline 
    \end{tabular}
    \caption[DM Halo TS Values]{
      TS values for each DM Halo Model likelihood ratio maximization fit.

      {\color{red}(Why is the 66.5 TeV one have -5.8 TS??)}
      
      {\color{red}(Comment on why they're all negative?? (the crab TS was positive!))}
    }
    \label{tab:tsvals}
  \end{table}

  With the previously mentioned observations and models, the likelihood function was maximized to find the best-fit model parameters.
  In Table \ref{tab:tsvals}, the TS values from each dark matter mass are shown.
  From Wilk's theorem {\color{red}(reference??)}, with a TS value greater than 20 would hint at the presence of a dark matter halo.
  As all of these are less than zero, it can be said that the null (no dark matter) hypothesis is statistically favorable.

  As there are 948 camera background models in each likelihood analysis, each with a free Normalisation and Spectral Index, these are histogrammed as a sanity check in Figure~\ref{fig:param_hist}.
  The Spectral Index distribution is centered on zero because the power law is applied to an existing (non-power-law) background shape.
  This means on average, the background models required little spectral hardening or softening.
  
  \begin{figure}[h]
    \begin{tabular}{ll}
      \includegraphics[scale=0.425]{images/likelihood_analysis/plot_pref.pdf} &
      \includegraphics[scale=0.425]{images/likelihood_analysis/plot_indx.pdf}
    \end{tabular}
    \caption{
      Histogram of the two free parameters in each of the 948 camera background models.
      Only the parameters from the $m_\chi\;=\;$\SI{45}{TeV} likelihood fit.
      {\color{red}(specify that these are just from one dm halo analysis!??)}
    }
    \label{fig:param_hist}
  \end{figure}
    
  To check that the other models are fitting the observed events, we can compare the observed vs modeled counts in different slices.
  In Figure~\ref{fig:gc_profile_gal_l}, the observed counts are shown compared to the final, likelihood-optimized modeled counts.
  The histogram bins are along a \ang{0.22}-wide slice along the galactic l axis, centered on Sgr A*'s galactic b coordinate.
  Purple histogram bins are the modeled counts from only the camera background models.
  Green histogram bins are the total modeled counts from the camera background models and the astrophysical models.
  Blue points with error bars are the observed counts in each bin, with poissonian errors.
  The feature where the observed counts are higher on the left and lower on the right is likely due to unaccounted-for elevation effects in the camera background models.
  The impact of this elevation effect is tested later in Section~\ref{sec:elevgradient}.
  
  \begin{figure}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{images/likelihood_analysis/profile_gal_l.eps}
    \llap{
      \makebox[13.75cm][l]{ % x position
        \raisebox{7cm}{     % y position
          {
            \setlength{\fboxsep }{0pt}
            \setlength{\fboxrule}{1pt}
            \fbox{
              \includegraphics[height=3cm]{images/likelihood_analysis/plot_brbbbar_45_0TeV_nfits1000_withpntsrc_regionprofl_counts.pdf}
            }
          }
        }
      }
    }
    \caption[Galactic Center Profile vs Galactic L]{
      Observed vs modeled counts in a slice of the sky parallel to the galactic L axis at $B=0$.
      Modeled counts from camera background models are shown in purple, total modeled counts are shown in green.
      The faint green line shows the total modeled counts at a higher spatial resolution, scaled to the histogram bins.
      Observed counts with poissonian errors are shown in blue.
      {\color{red}(fix inset to show regions!??)}
    }
    \label{fig:gc_profile_gal_l}
  \end{figure}

  Figure~\ref{fig:gc_profile_gal_b} is similar to Figure~\ref{fig:gc_profile_gal_l}, except it slices through Sgr A* along the galactic b axis,
  The feature where the observed counts are shifted to the left of the models is also visible here.

  \begin{figure}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{images/likelihood_analysis/profile_gal_b.eps}
    \llap{
      \makebox[13.75cm][l]{ % x position
        \raisebox{7cm}{   % y position
          {
            \setlength{\fboxsep }{0pt}
            \setlength{\fboxrule}{1pt}
            \fbox{
              \includegraphics[height=3cm]{images/likelihood_analysis/plot_brbbbar_45_0TeV_nfits1000_withpntsrc_regionprofb_counts.pdf}
            }
          }
        }
      }
    }
    \caption[Galactic Center Profile vs Galactic B]{
      Observed vs modeled counts in a slice of the sky parallel to the galactic B axis at $L=0$.
      Modeled counts from camera background models are shown in purple, total modeled counts are shown in green.
      Observed counts with poissonian errors are shown in blue.
    }
    \label{fig:gc_profile_gal_b}
  \end{figure}

  Figure~\ref{fig:gc_profile_energy} shows the same profile, except instead integrated over a \ang{0.2}$\times$\ang{0.2} galactic (l,b) square centered on Sgr A*.
  Again, purple is modeled camera-background counts, green is total modeled counts, and blue is observed counts.
  
  \begin{figure}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{images/likelihood_analysis/profile_energy.eps}
    \llap{
      \makebox[10.9cm][l]{ % x position
        \raisebox{7.15cm}{   % y position
          {
            \setlength{\fboxsep }{0pt}
            \setlength{\fboxrule}{1pt}
            \fbox{
              \includegraphics[height=2.9cm]{images/likelihood_analysis/plot_brbbbar_45_0TeV_nfits1000_withpntsrc_regionprofe_counts.pdf}
            }
          }
        }
      }
    }
    \caption[Galactic Center Profile vs Energy]{
      Observed vs modeled counts in a \ang{0.2}$\times$\ang{0.2} square bin centered on Sgr A* at energies from \SIrange{4}{70}{TeV}.
      Modeled counts from camera background models are shown in purple, total modeled counts are shown in green.
      Observed counts with poissonian errors are shown in blue.
    }
    \label{fig:gc_profile_energy}
  \end{figure}
  
  To check for any poorly modeled areas of the sky, a residual skymap is shown in Figure~\ref{fig:gc_resmap}.
  This calculates how significant the difference is between the observed and modeled number of counts.
  Each bin's significance is calculated with Equation~\ref{eqn:resmap_signif},
  
  % see http://cta.irap.omp.eu/ctools/users/reference_manual/csresmap.html
  \begin{equation}\label{eqn:resmap_signif}
    \text{Significance} = sign(D-M) \sqrt{ 2 \left ( D * \textrm{ln} \left ( \frac{D}{M} \right ) + M - D \right ) }
  \end{equation}
  
  where D is the number of observed counts, and M is the number of modeled counts.
  Equation~\ref{eqn:resmap_signif} is derived from the Li and Ma {\color{red}equation ??}. 
  In this plot, the deficiency of the radially-symmetric camera background models is apparent.
  Lighter/darker areas indicate places where there were more/less observed counts than the best-fit models predicted.
  
  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\textwidth]{images/likelihood_analysis/plot_resmap_algosignificance.eps}
    \caption[Galactic Center Residual Map]
    {
      Significance of each skymap bin's residual counts.
      See equation \ref{eqn:resmap_signif} for details.
      The dark upper-left and lighter lower-right areas are due to the the radially-symmetric background, when a non-radially symmetric one would have been more ideal.
    }
    \label{fig:gc_resmap}
  \end{figure}

  If the source and background models were perfectly known, there would still be statistical fluctuations in the observed counts, and the residual significances would form a gaussian distribution, shown as the green line in Figure~\ref{fig:gc_resmap_sighist}.
  However, the distribution of the pixel significances from Figure~\ref{fig:gc_resmap} is also shown in Figure~\ref{fig:gc_resmap_sighist}, and qualitativly not gaussian shaped {\color{red}(??)}.
  {\color{red}?? How well does it fit, and quantitativly how bad does it fit??}
  As a large number of bins are clustered around +2 and -2 signifiance, this hints that the residuals in Figure~\ref{fig:gc_resmap} are potentially not due solely from statistical noise, and that the background modeling could be improved.
  
  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\textwidth]{images/likelihood_analysis/plot_resmap_phist_algosignificance.eps}
    \caption[Galactic Center Residual Histogram]
    {
      Histogram of the residual skymap bin significances from Figure~\ref{fig:gc_resmap}, calculated with Equation~\ref{eqn:resmap_signif}.
      Only skymap bins within each observation's region of interest (the green circles in Figure~\ref{fig:gcfieldsofview}) are histogrammed.
    }
    \label{fig:gc_resmap_sighist}
  \end{figure}

  {\color{red}Plot of point source spectra as cross-check??}
  
\section{Upper Limit}
  From the dark matter flux Equation~\ref{eqn:dmflux}, an upper limit on the observed gamma-ray flux can be derived.
  When all other variables in Equation~\ref{eqn:dmflux} are fixed, a larger velocity-averaged crosssection always results in a larger gamma-ray flux, meaning the flux and crosssection can both be replaced by their upper limits ($x \rightarrow x \left |_{\textrm{ul}}$), as in Equation~\ref{eqn:ulim}.
  
  \begin{equation}\label{eqn:ulim}
    \frac{d\phi}{dE d\Omega} \left. \right |_{\textrm{ul}} = \frac{ \left \langle \sigma v \right \rangle \left | \right _{\textrm{ul}} }{8\pi m_{\chi}^{2}} \frac{dN_{\gamma}}{dE} \int \rho^{2} \; dl \;
  \end{equation}
  
  When calculating the upper limit with a log-likelihood ratio test, the confidance interval is used to calculate a likelihood upper limit, $L_{\textrm{ul}}$.
  This likelihood upper limit is calculated in Equation~\ref{eqn:ulim_deltaL}, where $\alpha$ is the confidance interval as a fraction of 1.

  \begin{equation}\label{eqn:ulim_deltaL}
    L_{\textrm{ul}} = L_{\textrm{max}} - \left ( \textrm{erf}^{-1} \left ( \alpha \right ) \right )^2
  \end{equation}
  {\color{red}(where does this equation come from? find ctulimit's source??)}

  A 95\% confidence interval is desired, so $\alpha$ is set to 0.95 for the upper limit calculation in this section.
  Then the magnitude of the dark matter halo model's flux $\Phi$ is fixed at one magnitude, the likelihood method finds the best fit values for the remaining free parameters, and the likelihood is recalculated.
  This is repeated until the the likelihood is found to be within a certain tolerance of $L_{\textrm{ul}}$.
  Once the upper limit model flux $\Phi$ is found, then with Equation~\ref{eqn:ulim} the crosssection upper limit can be calculated.

  From the 9 WIMP masses tested in Section~\ref{sec:dmlike}, the calculated upper limits are shown in Figure~\ref{fig:ulim}.
  These show the upper limits calculated from this work, along with several comparison upper limits.
  The pink upper limit is from 216 hours of VERITAS observations of several dwarf spheroidal galaxies, stacked into a single likelihood analysis~\cite{verdsphul}.
  The green upper limit is from 254 hours of H.E.S.S. observations of the Galactic Center~\cite{hessgcul}.
  The yellow upper limit is from a combined Fermi-Magic likelihood analysis using observations from several dwarf satellite galaxies~\cite{fermagicul}.
  Fermi and MAGIC spent a total of 6 years and 158 hours observing these galaxies, respectivly.
  The light blue line is the relic cross-section~\cite{updatedWIMPRelicCrossSection}.
  
  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\textwidth]{images/ulimit/ulimit.pdf}
    \caption[Dark Matter Upper Limit Plot]{
      Dark Matter upper limits on velocity-averaged cross-sections.
      The results from this work are in dark blue, while pink, green, and yellow show published limits from VERITAS, H.E.S.S., and a joint Fermi-MAGIC analysis.
      Data values from this work's upper limits are in Appendix Table \ref{tab:ulimvals}.
      The light blue line is the relic crosssection.
      {\color{red}(Do I need to incorporate systematic uncertainties in this, or is this already accounted for by the energy migration matrices??)}
      {\color{red}(Refresh plot!??)}
    }
    \label{fig:ulim}
  \end{figure}
  
\FloatBarrier

\section{Impact of Elevation Gradient}\label{sec:elevgradient}

  The residual map in Figure~\ref{fig:gc_resmap} has a gradient that is visible to the naked eye, going from the lower right to the upper left.
  This means that the radial camera background models are not accurately fitting the shape of the observed events.
  The direction of the residual gradient aligns with the elevation gradient in the camera, visible in Figures \ref{fig:back_lowelev29} D and E, and Figures \ref{fig:back_lowelev26} D and E.
  To check that this elevation gradient has a negligible effect on the upper limit result in Figure~\ref{fig:ulim}, the likelihood analysis was redone with an elevation gradient.
  As this analysis is limited to the energy range \SIrange{4}{70}{TeV}, the elevation gradient will increase the number of events detected at the bottom of the camera, due to the thicker atmosphere providing increased interaction mass.
  And, since the Galactic Center is visible almost directly south at an azimuth of \ang{193}, the elevation axis is almost parallel to the Declination axis.
  A gradient of 5\%/\degree{} was chosen due to the relative differences between the total modeled counts and the observed counts in Figures \ref{fig:gc_profile_gal_l} and \ref{fig:gc_profile_gal_b}.
  An example background is shown in Figure~\ref{fig:bkg_flatvsgrad}, before and after the gradient is applied.
  
  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.98\textwidth]{images/background_gradient/gradientcomparison.pdf}
    \caption[Background Gradient Comparison]{
      The background models for run 68348, minutes 16-24.
      Left is the original radial background, centered \ang{0.5} North of the Galactic Center.
      Right is the same background, after applying the 5\%/\degree{} gradient along the elevation axis.
    }
    \label{fig:bkg_flatvsgrad}
  \end{figure}
  
  After these were applied to all background models, the likelihood analysis was re-ran for the \SI{10}{\TeV} WIMP mass.
  The resulting upper limit with the gradient was only 0.1\% different than with a simple radial background, indicating the upper limits are not very sensitive to the background models.
  % on warp-zeuthen.desy.de:
  % cd $VERIPY/thesis/analysis/dm_plus_pnt/
  %
  % no gradient:
  % $ grep "crosssection_ul" logs/statistics.brbbbar.10.0TeV.nfits1000.withpntsrc.txt 
  % [1.6279762902243517e-24, 'cm^3/s']
  %
  % with gradient:
  % $ grep "crosssection_ul" with_elev_gradient/logs/statistics.brbbbar.10.0TeV.nfits1000.withpntsrc.txt 
  % [1.6280879530415155e-24, 'cm^3/s']
  This likely stems from the fact that the test statistic is a ratio of likelihoods of two model groups, both of which contain the background models.
  Thus, any strong effect in the background models will appear in the top and bottom of the likelihood ratio, and partially cancel.
  
  
  
  

\section{Systematic Errors}  

Stuff?


